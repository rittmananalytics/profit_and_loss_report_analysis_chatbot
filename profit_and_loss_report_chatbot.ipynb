{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2024 Rittman Analytics ltd\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "6QHj_OzW_7pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# P&L Query Self-Learning Chatbot Prototype with RAG and SQL Agent\n",
        "\n",
        "## Overview\n",
        "\n",
        "This Jupyter notebook implements an advanced question-answering system for Profit and Loss (P&L) data analysis. It combines Retrieval-Augmented Generation (RAG), vector store queries, and SQL-based data retrieval to provide comprehensive and accurate answers to financial questions.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "1. Hybrid RAG and SQL-based querying\n",
        "2. Dynamic decision-making between RAG and SQL approaches\n",
        "3. Feedback loop for continuous improvement\n",
        "4. Vector store for storing and retrieving successful Q&A pairs\n",
        "5. Flexible time period handling for financial queries\n",
        "\n",
        "## System Architecture\n",
        "\n",
        "### 1. Vector Stores\n",
        "\n",
        "The system uses two vector stores:\n",
        "- `vector_store`: Stores pre-created P&L report analyses\n",
        "- `qa_vector_store`: Stores successful question-answer pairs\n",
        "\n",
        "Both vector stores are implemented using BigQuery and the `BigQueryVectorStore` class from LangChain.\n",
        "\n",
        "### 2. Embedding Model\n",
        "\n",
        "We use the `VertexAIEmbeddings` model to generate embeddings for both the pre-created analyses and the Q&A pairs.\n",
        "\n",
        "### 3. Language Model\n",
        "\n",
        "The system uses OpenAI's GPT-4 model (via `ChatOpenAI`) for natural language understanding and generation.\n",
        "\n",
        "### 4. SQL Database\n",
        "\n",
        "The P&L data is stored in BigQuery tables, accessed through SQLAlchemy.\n",
        "\n",
        "## Workflow\n",
        "\n",
        "1. **Question Input**: The user inputs a question about P&L data.\n",
        "\n",
        "2. **RAG vs SQL Decision**:\n",
        "   - The `should_query_vector_store` function determines whether to use RAG or SQL-based querying.\n",
        "   - It considers the question content, time frame, and available data in the vector store.\n",
        "\n",
        "3. **RAG Approach** (if chosen):\n",
        "   - The system queries the `vector_store` for relevant pre-created analyses.\n",
        "   - It uses the `similarity_search` method to find the most relevant document.\n",
        "   - The retrieved content is summarized and presented as the answer.\n",
        "\n",
        "4. **SQL Approach** (if chosen or if RAG fails):\n",
        "   - The system extracts relevant time periods and financial categories from the question.\n",
        "   - It constructs a SQL query using the `SQLDatabaseToolkit` and `create_sql_agent` from LangChain.\n",
        "   - The query is executed against the BigQuery database, and the results are processed to form an answer.\n",
        "\n",
        "5. **Answer Evaluation**:\n",
        "   - The `evaluate_answer_relevance` function uses the language model to assess the relevance and quality of the generated answer.\n",
        "\n",
        "6. **User Feedback**:\n",
        "   - The system asks the user if the answer is sufficient.\n",
        "   - If not, it requests feedback on how to improve the answer.\n",
        "\n",
        "7. **Question Refinement**:\n",
        "   - If the user is not satisfied, the system uses the language model to generate an improved question based on the feedback.\n",
        "   - The process repeats with the refined question (up to a maximum of 3 iterations).\n",
        "\n",
        "8. **Storing Successful Q&A Pairs**:\n",
        "   - When a user is satisfied with an answer, the Q&A pair is stored in the `qa_vector_store`.\n",
        "   - This stored pair can be used to inform future similar questions.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "### 1. `extract_time_periods` Function\n",
        "Identifies and extracts various time periods (years, months, quarters) mentioned in the question.\n",
        "\n",
        "### 2. `construct_date_filter` Function\n",
        "Constructs SQL date filters based on the extracted time periods.\n",
        "\n",
        "### 3. `ask_question` Function\n",
        "The core function that orchestrates the question-answering process, including the decision between RAG and SQL approaches.\n",
        "\n",
        "### 4. `ask_question_with_feedback_and_learning` Function\n",
        "Implements the feedback loop and question refinement process.\n",
        "\n",
        "### 5. `store_successful_qa` Function\n",
        "Stores successful Q&A pairs in the vector store for future reference.\n",
        "\n",
        "## Continuous Learning\n",
        "\n",
        "The system improves over time through two mechanisms:\n",
        "1. Storing successful Q&A pairs in the vector store, which can be retrieved for similar future questions.\n",
        "2. The feedback loop, which allows for question refinement based on user input.\n",
        "\n",
        "## Usage\n",
        "\n",
        "1. Set up the required Google Cloud and OpenAI credentials.\n",
        "2. Run the notebook cells in order to initialize all components.\n",
        "3. Use the `main` function to start an interactive Q&A session about P&L data.\n",
        "\n",
        "## Customization\n",
        "\n",
        "The system can be customized by:\n",
        "- Modifying the `vector_store_content_description` to match specific P&L data structures.\n",
        "- Adjusting the SQL views and table names in the `determine_view` function.\n",
        "- Expanding the `extract_time_periods` function to handle more complex date formats."
      ],
      "metadata": {
        "id": "5P92ku-Z_z8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bs4 langchain langchain-community langchain-google-community langchain_google_vertexai langchain-openai openai chromadb tiktoken tabulate sqlalchemy sqlalchemy-bigquery google-cloud-bigquery"
      ],
      "metadata": {
        "id": "qGLdYxA3Wq6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.cloud import bigquery\n",
        "from google.api_core import exceptions\n",
        "from sqlalchemy import *\n",
        "from sqlalchemy.engine import create_engine\n",
        "from sqlalchemy.schema import *\n",
        "from langchain.agents import create_sql_agent\n",
        "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
        "from langchain.sql_database import SQLDatabase\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain, LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "from langchain_google_community import BigQueryVectorStore\n",
        "import re\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from langchain.docstore.document import Document\n"
      ],
      "metadata": {
        "id": "1UDkrP8EW2_i",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1727694931361,
          "user_tz": -60,
          "elapsed": 120,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_account_file = \"/content/ra-development-bf14f75f80cd.json\"\n",
        "project = \"ra-development\"\n",
        "dataset = \"analytics_finance_demo\"\n",
        "location = \"europe-west2\"\n",
        "sqlalchemy_url = f'bigquery://{project}/{dataset}?credentials_path={service_account_file}'\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPEN_AI_API_KEY>\"\n",
        "\n",
        "vector_store_content_description = \"\"\"\n",
        "The vector store contains pre-created analysis with the following information:\n",
        "1. Key Metrics\n",
        "   - Revenue, Overheads, Cost of Delivery, Gross & Net Profit, Retained Earnings net amounts\n",
        "   - Account Sub-Category and Account Group net amounts\n",
        "2. Key Metrics Calculation:\n",
        "   - Month-over-month, year-to-date, and budget variance calculations\n",
        "   - Gross margin percentages\n",
        "3. Significant Transaction Identification:\n",
        "   - Transactions exceeding a certain percentage threshold of the total account group amount\n",
        "   - Cancelling transactions are identified and excluded\n",
        "   - Context for each significant transaction (new or changed from previous month)\n",
        "4. Overhead Trend Analysis:\n",
        "   - Monthly growth rates for overhead categories over the last 6 months\n",
        "   - Account groups with significant average monthly growth (> 10%)\n",
        "5. Identification of New Repeating (Recurring) Transactions:\n",
        "   - Transactions with the same description appearing consistently over the last 3 months\n",
        "\n",
        "The analysis covers the last three months and the current year-to-date at summary level for category, subcategory, and account group levels.\n",
        "\"\"\"\n",
        "\n",
        "# Create a BigQuery client\n",
        "client = bigquery.Client.from_service_account_json(service_account_file)\n",
        "\n",
        "# Initialize embedding model\n",
        "embedding_model = VertexAIEmbeddings(\n",
        "    model_name=\"textembedding-gecko@latest\",\n",
        "    project=project\n",
        ")\n",
        "\n",
        "# Initialize BigQueryVectorStore containing P&L report analysis texts\n",
        "vector_store = BigQueryVectorStore(\n",
        "    project_id=project,\n",
        "    dataset_name=dataset,\n",
        "    table_name=\"pl_reports_vector_storage\",\n",
        "    location=location,\n",
        "    embedding=embedding_model,\n",
        ")\n",
        "\n",
        "def create_successful_qa_table():\n",
        "    client = bigquery.Client()\n",
        "    table_id = f\"{project}.{dataset}.successful_qa_pairs\"\n",
        "\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"text\", \"STRING\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"embedding\", \"FLOAT64\", mode=\"REPEATED\"),\n",
        "        bigquery.SchemaField(\"id\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    ]\n",
        "\n",
        "    table = bigquery.Table(table_id, schema=schema)\n",
        "    table.clustering_fields = [\"id\"]\n",
        "\n",
        "    try:\n",
        "        table = client.create_table(table)\n",
        "        print(f\"Created table {table.project}.{table.dataset_id}.{table.table_id}\")\n",
        "    except exceptions.Conflict:\n",
        "        print(f\"Table {table_id} already exists.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while creating the table: {str(e)}\")\n",
        "\n",
        "# Initialize a new vector store for storing successful Q&A pairs\n",
        "def initialize_qa_vector_store():\n",
        "    create_successful_qa_table()\n",
        "    return BigQueryVectorStore(\n",
        "        project_id=project,\n",
        "        dataset_name=dataset,\n",
        "        table_name=\"successful_qa_pairs\",\n",
        "        location=location,\n",
        "        embedding=embedding_model,\n",
        "    )\n",
        "\n",
        "qa_vector_store = initialize_qa_vector_store()\n",
        "\n",
        "def load_vector_storage():\n",
        "    query = f\"\"\"\n",
        "    SELECT date_month as month, report_analysis, invoice_analysis, recurring_payments_analysis\n",
        "    FROM `{project}.{dataset}.pl_reports`\n",
        "    \"\"\"\n",
        "    df = client.query(query).to_dataframe()\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        month = row['month']\n",
        "        for analysis_type in ['report_analysis', 'invoice_analysis', 'recurring_payments_analysis']:\n",
        "            text = row[analysis_type]\n",
        "            metadata = {\n",
        "                'month': month,\n",
        "                'analysis_type': analysis_type\n",
        "            }\n",
        "            vector_store.add_texts([text], metadatas=[metadata])\n",
        "\n",
        "    print(\"Vector storage loaded successfully.\")\n",
        "\n",
        "def get_available_months():\n",
        "    query = f\"\"\"\n",
        "    SELECT DISTINCT month as month\n",
        "    FROM `{project}.{dataset}.pl_reports_vector_storage`\n",
        "    ORDER BY month DESC\n",
        "    \"\"\"\n",
        "    df = client.query(query).to_dataframe()\n",
        "    return df['month'].tolist()\n",
        "\n",
        "# Add this function to determine the valid time range\n",
        "def get_valid_time_range(available_months):\n",
        "    if not available_months:\n",
        "        return None, None\n",
        "\n",
        "    latest_month = max(available_months)\n",
        "    earliest_month = min(available_months)\n",
        "\n",
        "    # Calculate the start of the year for the latest month\n",
        "    year_start = latest_month.replace(month=1, day=1)\n",
        "\n",
        "    # The valid range includes two months before the earliest available month\n",
        "    valid_start = (earliest_month - relativedelta(months=2)).replace(day=1)\n",
        "\n",
        "    return valid_start, latest_month\n",
        "\n",
        "def create_lookup_tables():\n",
        "    query = f\"\"\"\n",
        "    SELECT DISTINCT\n",
        "        account_report_group,\n",
        "        account_report_sub_category,\n",
        "        account_category\n",
        "    FROM `{project}.{dataset}.profit_and_loss_report_account_group`\n",
        "    \"\"\"\n",
        "    df = client.query(query).to_dataframe()\n",
        "\n",
        "    lookups = {\n",
        "        'group': defaultdict(list),\n",
        "        'sub_category': defaultdict(list),\n",
        "        'category': defaultdict(list)\n",
        "    }\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        group = row['account_report_group']\n",
        "        sub_category = row['account_report_sub_category']\n",
        "        category = row['account_category']\n",
        "\n",
        "        lookups['group'][group.lower()].append(group)\n",
        "        lookups['sub_category'][sub_category.lower()].append(sub_category)\n",
        "        lookups['category'][category.lower()].append(category)\n",
        "\n",
        "    return lookups\n",
        "\n",
        "lookups = create_lookup_tables()\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "db = SQLDatabase.from_uri(sqlalchemy_url)\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "conversation_chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
        "agent_executor = create_sql_agent(\n",
        "    llm=llm,\n",
        "    toolkit=toolkit,\n",
        "    verbose=False,\n",
        "    top_k=1000,\n",
        ")\n",
        "\n",
        "def find_matching_values(question, lookups):\n",
        "    words = question.lower().split()\n",
        "    matches = {\n",
        "        'group': set(),\n",
        "        'sub_category': set(),\n",
        "        'category': set()\n",
        "    }\n",
        "\n",
        "    for word in words:\n",
        "        for key in lookups:\n",
        "            if word in lookups[key]:\n",
        "                matches[key].update(lookups[key][word])\n",
        "\n",
        "    return matches\n",
        "\n",
        "def construct_filter_clause(matches):\n",
        "    clauses = []\n",
        "    for key, values in matches.items():\n",
        "        if values:\n",
        "            column = \"account_report_group\" if key == \"group\" else f\"account_{key}\"\n",
        "            quoted_values = [\"'{0}'\".format(v) for v in values]\n",
        "            clause = \"{0} IN ({1})\".format(column, \", \".join(quoted_values))\n",
        "            clauses.append(clause)\n",
        "\n",
        "    return \" AND \".join(clauses) if clauses else \"\"\n",
        "\n",
        "def determine_view(matches):\n",
        "    if matches['group']:\n",
        "        return \"profit_and_loss_report_account_group_xa\"\n",
        "    elif matches['sub_category']:\n",
        "        return \"profit_and_loss_report_sub_categories_xa\"\n",
        "    else:\n",
        "        return \"profit_and_loss_report_categories_xa\"\n",
        "\n",
        "def extract_time_periods(question):\n",
        "    \"\"\"Extract time periods mentioned in the question.\"\"\"\n",
        "    # Patterns for various time formats\n",
        "    year_pattern = r'\\b(20\\d{2})\\b'\n",
        "    month_year_pattern = r'\\b(Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+(\\d{4})\\b'\n",
        "    quarter_pattern = r'\\b(Q[1-4])\\s+(\\d{4})\\b'\n",
        "\n",
        "    time_periods = []\n",
        "\n",
        "    # Extract years\n",
        "    years = re.findall(year_pattern, question)\n",
        "    for year in years:\n",
        "        time_periods.append(('year', int(year)))\n",
        "\n",
        "    # Extract month-year combinations\n",
        "    month_years = re.findall(month_year_pattern, question, re.IGNORECASE)\n",
        "    for month, year in month_years:\n",
        "        date = parser.parse(f\"{month} {year}\")\n",
        "        time_periods.append(('month', date))\n",
        "\n",
        "    # Extract quarters\n",
        "    quarters = re.findall(quarter_pattern, question)\n",
        "    for quarter, year in quarters:\n",
        "        quarter_num = int(quarter[1])\n",
        "        start_month = (quarter_num - 1) * 3 + 1\n",
        "        time_periods.append(('quarter', parser.parse(f\"{year}-{start_month:02d}-01\")))\n",
        "\n",
        "    return time_periods\n",
        "\n",
        "def construct_date_filter(time_periods):\n",
        "    \"\"\"Construct a SQL date filter based on extracted time periods.\"\"\"\n",
        "    filters = []\n",
        "    for period_type, date in time_periods:\n",
        "        if period_type == 'year':\n",
        "            filters.append(f\"EXTRACT(YEAR FROM date_month) = {date}\")\n",
        "        elif period_type == 'month':\n",
        "            filters.append(f\"date_month = DATE('{date.strftime('%Y-%m-%d')}')\")\n",
        "        elif period_type == 'quarter':\n",
        "            quarter_end = date + relativedelta(months=2, day=31)\n",
        "            filters.append(f\"date_month BETWEEN DATE('{date.strftime('%Y-%m-%d')}') AND DATE('{quarter_end.strftime('%Y-%m-%d')}')\")\n",
        "\n",
        "    return \" OR \".join(filters) if filters else \"\"\n",
        "\n",
        "def strip_html(html_content):\n",
        "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "def summarize_content(question, content):\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"question\", \"content\"],\n",
        "        template=\"\"\"Given the following question and content, provide a concise summary of the content that is directly relevant to answering the question.\n",
        "        Ignore any information that doesn't pertain to the question. All amounts should be stated in GBP (Â£).\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Content: {content}\n",
        "\n",
        "        Relevant Summary:\"\"\"\n",
        "    )\n",
        "\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    summary = chain.run(question=question, content=content)\n",
        "    return summary.strip()\n",
        "\n",
        "def should_query_vector_store(question):\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"question\", \"content_description\", \"valid_time_range\"],\n",
        "        template=\"\"\"Given the following question and description of the content in a vector store,\n",
        "        determine if the vector store is likely to contain information that can answer the question.\n",
        "        Consider the following:\n",
        "        1. Does the question ask about any of the key metrics or analyses mentioned in the content description?\n",
        "        2. Does the question fall within the time frame covered by the vector store? Valid time range: {valid_time_range}\n",
        "        3. Is the level of detail requested (category, subcategory, account group) available in the vector store?\n",
        "\n",
        "        Respond with 'Yes' if the vector store is likely to contain relevant information, or 'No' if it's unlikely or unclear.\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Vector Store Content Description:\n",
        "        {content_description}\n",
        "\n",
        "        Decision (Yes/No):\n",
        "        Explanation:\"\"\"\n",
        "    )\n",
        "\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    response = chain.run(question=question,\n",
        "                         content_description=vector_store_content_description,\n",
        "                         valid_time_range=f\"{valid_time_range[0].strftime('%B %Y')} to {valid_time_range[1].strftime('%B %Y')}\")\n",
        "\n",
        "\n",
        "    # Extract the decision from the response\n",
        "    decision_match = re.search(r'Decision \\(Yes/No\\):\\s*(Yes|No)', response, re.IGNORECASE)\n",
        "    if decision_match:\n",
        "        decision = decision_match.group(1).lower()\n",
        "        return decision == 'yes'\n",
        "    else:\n",
        "        # If no clear decision is found in the expected format, look for a 'Yes' at the beginning of the response\n",
        "        if response.strip().lower().startswith('yes'):\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "def evaluate_answer_relevance(question, answer):\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"question\", \"answer\"],\n",
        "        template=\"\"\"Given the following question and answer, evaluate how well the answer addresses the question.\n",
        "        Provide a relevance score as a percentage and a brief explanation.\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Answer: {answer}\n",
        "\n",
        "        Relevance Score (0-100%):\n",
        "        Explanation:\"\"\"\n",
        "    )\n",
        "\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    response = chain.run(question=question, answer=answer)\n",
        "    return response\n",
        "\n",
        "def ask_question(question, context=\"\"):\n",
        "    time_periods = extract_time_periods(question)\n",
        "\n",
        "    if should_query_vector_store(question):\n",
        "        month = time_periods[0][1] if time_periods and time_periods[0][0] in ['month', 'quarter'] else None\n",
        "        filter_dict = {\"month\": month.strftime('%Y-%m-%d') if month else None}\n",
        "        relevant_docs = vector_store.similarity_search(question, k=1, filter=filter_dict)\n",
        "\n",
        "        if relevant_docs:\n",
        "            pre_created_answer = relevant_docs[0].page_content\n",
        "            stripped_answer = strip_html(pre_created_answer)\n",
        "            summarized_answer = summarize_content(question, stripped_answer)\n",
        "            answer = f\"Based on the pre-created analysis: {summarized_answer}\"\n",
        "        else:\n",
        "            answer = \"No relevant pre-created analysis found. Falling back to SQL query.\"\n",
        "    else:\n",
        "        answer = \"The question cannot be answered by pre-created analysis. Using SQL query.\"\n",
        "\n",
        "    if \"Using SQL query\" in answer:\n",
        "        matches = find_matching_values(question, lookups)\n",
        "        filter_clause = construct_filter_clause(matches)\n",
        "        view_name = determine_view(matches)\n",
        "\n",
        "        date_filter = construct_date_filter(time_periods)\n",
        "\n",
        "        if date_filter:\n",
        "            filter_clause = f\"({date_filter}) AND {filter_clause}\" if filter_clause else date_filter\n",
        "\n",
        "        instruction = f\"\"\"You are a knowledgeable finance data analyst working for Rittman Analytics.\n",
        "        Use the `{project}.{dataset}.{view_name}` view to answer this question.\n",
        "        Use the following SQL filter clause in your query: {filter_clause}\n",
        "        When calculating revenue or any other financial metrics, make sure to aggregate the values for the entire time period(s) mentioned in the question (which may be months, quarters, or years).\n",
        "        If multiple time periods are mentioned, provide a comparison between them.\n",
        "        Please construct and execute a SQL query to answer the question, making sure to include the filter clause.\n",
        "        Do not include markdown-style triple backticks in the SQL you generate and try to use or validate.\n",
        "\n",
        "        {context}\n",
        "\n",
        "        Question is: {question}\n",
        "        \"\"\"\n",
        "\n",
        "        answer = agent_executor.run(instruction)\n",
        "\n",
        "    relevance_evaluation = evaluate_answer_relevance(question, answer)\n",
        "\n",
        "    return f\"{answer}\\n\\nRelevance Evaluation:\\n{relevance_evaluation}\"\n",
        "\n",
        "def extract_dates_from_question(question):\n",
        "    \"\"\"Extract all years mentioned in the question.\"\"\"\n",
        "    year_pattern = r'\\b(20\\d{2})\\b'\n",
        "    return list(set(re.findall(year_pattern, question)))\n",
        "\n",
        "def get_similar_qa(question: str, k: int = 3):\n",
        "    \"\"\"Retrieve similar Q&A pairs from the vector store.\"\"\"\n",
        "    similar_qa = qa_vector_store.similarity_search(question, k=k)\n",
        "\n",
        "    # Convert the results to the expected format\n",
        "    converted_results = []\n",
        "    for i, item in enumerate(similar_qa):\n",
        "        if isinstance(item, Document):\n",
        "            # If the item is already a Document, use it as is\n",
        "            converted_results.append(item)\n",
        "            print(f\"Document {i+1} (already Document):\")\n",
        "            print(f\"  Page content: {item.page_content[:100]}...\")\n",
        "            print(f\"  Metadata: {item.metadata}\")\n",
        "        else:\n",
        "            # If the item is a dict or has a different structure, create a new Document\n",
        "            text = item.get('text', '')  # Use 'text' instead of 'content'\n",
        "            metadata = {key: value for key, value in item.items() if key != 'text'}\n",
        "            doc = Document(page_content=text, metadata=metadata)\n",
        "            converted_results.append(doc)\n",
        "            print(f\"Document {i+1} (converted to Document):\")\n",
        "            print(f\"  Page content: {text[:100]}...\")\n",
        "            print(f\"  Metadata: {metadata}\")\n",
        "\n",
        "    return converted_results\n",
        "\n",
        "def format_similar_qa(similar_qa):\n",
        "    \"\"\"Format similar Q&A pairs for inclusion in the prompt.\"\"\"\n",
        "    formatted = \"Similar Q&A pairs from past interactions:\\n\\n\"\n",
        "    for i, doc in enumerate(similar_qa, 1):\n",
        "        formatted += f\"{i}. {doc.page_content}\\n\\n\"\n",
        "    return formatted\n",
        "\n",
        "def ask_question_with_feedback_and_learning(question: str, max_iterations: int = 3) -> str:\n",
        "    iteration = 0\n",
        "    while iteration < max_iterations:\n",
        "        # Retrieve similar Q&A pairs\n",
        "        similar_qa = get_similar_qa(question)\n",
        "\n",
        "        # Prepare the context with similar Q&A pairs\n",
        "        context = format_similar_qa(similar_qa)\n",
        "\n",
        "        # Use the modified ask_question function with context\n",
        "        answer = ask_question(question, context)\n",
        "        print(f\"\\nAnswer (Iteration {iteration + 1}):\\n{answer}\")\n",
        "\n",
        "        user_satisfied = input(\"\\nDid this answer your question sufficiently? (yes/no): \").lower().strip()\n",
        "\n",
        "        if user_satisfied == 'yes':\n",
        "            # Store the successful Q&A pair\n",
        "            store_successful_qa(question, answer)\n",
        "            return answer\n",
        "\n",
        "        feedback = input(\"Please provide feedback on how the answer could be improved: \")\n",
        "\n",
        "        # Use LLM to analyze feedback and improve the question\n",
        "        improve_prompt = PromptTemplate(\n",
        "            input_variables=[\"original_question\", \"answer\", \"feedback\", \"context\"],\n",
        "            template=\"\"\"Given the original question, the provided answer, user feedback, and similar Q&A pairs from past interactions,\n",
        "            please suggest an improved version of the question that addresses the user's concerns.\n",
        "\n",
        "            Original Question: {original_question}\n",
        "\n",
        "            Provided Answer: {answer}\n",
        "\n",
        "            User Feedback: {feedback}\n",
        "\n",
        "            {context}\n",
        "\n",
        "            Improved Question:\"\"\"\n",
        "        )\n",
        "\n",
        "        improve_chain = LLMChain(llm=llm, prompt=improve_prompt)\n",
        "        improved_question = improve_chain.run(\n",
        "            original_question=question,\n",
        "            answer=answer,\n",
        "            feedback=feedback,\n",
        "            context=context\n",
        "        )\n",
        "\n",
        "        print(f\"\\nImproved question based on feedback: {improved_question}\")\n",
        "\n",
        "        question = improved_question  # Update the question for the next iteration\n",
        "        iteration += 1\n",
        "\n",
        "    return \"I apologize, but I couldn't provide a satisfactory answer within the maximum number of iterations. Please try rephrasing your question or contact support for further assistance.\"\n",
        "\n",
        "\n",
        "def store_successful_qa(question: str, answer: str):\n",
        "    \"\"\"Store a successful question-answer pair in the vector store.\"\"\"\n",
        "    qa_pair = f\"Q: {question}\\nA: {answer}\"\n",
        "\n",
        "    # Generate embedding for the QA pair\n",
        "    embedded_vector = embedding_model.embed_query(qa_pair)\n",
        "\n",
        "    # Prepare the row to be inserted\n",
        "    row = {\n",
        "        'text': qa_pair,\n",
        "        'embedding': embedded_vector,\n",
        "        'id': str(uuid.uuid4())  # Generate a unique ID\n",
        "    }\n",
        "\n",
        "    # Insert the row into BigQuery\n",
        "    client = bigquery.Client()\n",
        "    table_id = f\"{project}.{dataset}.successful_qa_pairs\"\n",
        "\n",
        "    errors = client.insert_rows_json(table_id, [row])\n",
        "\n",
        "    if errors == []:\n",
        "        print(\"Successful Q&A pair stored in vector store.\")\n",
        "    else:\n",
        "        print(f\"Errors occurred while storing Q&A pair: {errors}\")\n",
        "\n",
        "\n",
        "\n",
        "def main(reload_vector_storage=False):\n",
        "    global valid_time_range, vector_store_content_description\n",
        "\n",
        "    if reload_vector_storage:\n",
        "        load_vector_storage()\n",
        "\n",
        "    available_months = get_available_months()\n",
        "    valid_time_range = get_valid_time_range(available_months)\n",
        "\n",
        "    # Update the vector_store_content_description with the actual time range\n",
        "    vector_store_content_description += f\"\"\"\n",
        "    The analysis covers the period from {valid_time_range[0].strftime('%B %Y')} to {valid_time_range[1].strftime('%B %Y')}.\n",
        "    For each month in this range, the analysis includes data for that month, comparisons to the two previous months, and year-to-date figures.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Hi! Ask me a question about our company's profit and loss data\")\n",
        "    while True:\n",
        "        question = input(\"\\nYour question (or type 'QUIT' to exit): \")\n",
        "        if question.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        final_answer = ask_question_with_feedback_and_learning(question)\n",
        "        print(f\"\\nFinal Answer: {final_answer}\")\n",
        "        print(\"\\n---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(reload_vector_storage=False)  # Set to True to reload vector storage"
      ],
      "metadata": {
        "id": "MeWPaAlSWrvj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}